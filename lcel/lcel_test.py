from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?")
print(prompt)
"""
input_variables=[] template='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?'
"""

prompt = PromptTemplate.from_template("{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?")
print(prompt)
"""
input_variables=['country'] template='{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?'
"""

from gpt_llm import openai_chatgpt_llm

model = openai_chatgpt_llm.generate_llm()

#################

chain = prompt | model
# chain.invoke()
""" input ê°’ì´ ë¹ ì¡ŒìŒ!
TypeError: RunnableSequence.invoke() missing 1 required positional argument: 'input'
"""
answer = chain.invoke({"country": "ë©•ì‹œì½”"})
print(answer)
"""
content='ë©•ì‹œì½”ì˜ ìˆ˜ë„ëŠ” ë©•ì‹œì½”ì‹œí‹°ì…ë‹ˆë‹¤.' response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 21, 'total_tokens': 41}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-cf09fb59-f176-4b2f-ba9a-db23042777fb-0' usage_metadata={'input_tokens': 21, 'output_tokens': 20, 'total_tokens': 41}
"""

########## Runnableì— ëŒ€í•´ ì•Œì•„ë³´ì ###########

from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableParallel,
    RunnableLambda,
)
########## RunnablePassthrough ###########

# chain.invoke("ë©•ì‹œì½”") # ì—ëŸ¬
# ì´ë ‡ê²Œ ë³€ìˆ˜ë¥¼ ì£¼ê³ ì í•  ë•Œ, dictionary ê°’ìœ¼ë¡œ ì£¼ì§€ ì•Šê³  ë°”ë¡œ í• ë‹¹í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ë¬¸ë²•
# í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ì—¬ëŸ¬ í˜•íƒœë¡œ ì„¸íŒ…í•  ìˆ˜ ìˆë‹¤.
passthrough_chain = {"country": RunnablePassthrough()} | prompt | model
answer = passthrough_chain.invoke("ë©•ì‹œì½”")
print(answer)
"""
content='ë©•ì‹œì½”ì˜ ìˆ˜ë„ëŠ” ë©•ì‹œì½”ì‹œí‹°(Mexico City)ì…ë‹ˆë‹¤.' response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 21, 'total_tokens': 46}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d59ef9aa-108e-4e16-a6d1-8eb26d76fdb6-0' usage_metadata={'input_tokens': 21, 'output_tokens': 25, 'total_tokens': 46}
"""
# 2ê°œë¼ë©´
# chain = {"document": retriever, "question": RunnablePassthrough()} | prompt | model
# ì´ëŸ° ì‹ìœ¼ë¡œ ê°€ëŠ¥

########## RunnableParallel ###########

# # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•œê²Œ í•´ì¤Œ

prompt2 = PromptTemplate.from_template("{country}ì˜ ì¸êµ¬ëŠ” ëª‡ëª…ì´ì•¼?")

chain1 = {"country": RunnablePassthrough()} | prompt | model
chain2 = {"country": RunnablePassthrough()} | prompt2 | model

map_chain = RunnableParallel(a=chain1, b=chain2)
answer = map_chain.invoke("ëŒ€í•œë¯¼êµ­")
print(answer)
"""
{
    'a': AIMessage(content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 22, 'total_tokens': 38}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e0f47726-b162-4cb7-9c2b-a30bbbdb0d67-0', usage_metadata={'input_tokens': 22, 'output_tokens': 16, 'total_tokens': 38}), 
    'b': AIMessage(content='2021ë…„ 7ì›” ê¸°ì¤€ ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ëŠ” ì•½ 5ì²œ 130ë§Œ ëª…ì…ë‹ˆë‹¤.', response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 23, 'total_tokens': 57}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-02aeb56b-7499-4999-9b36-7eaf3b3ef7c4-0', usage_metadata={'input_tokens': 23, 'output_tokens': 34, 'total_tokens': 57})
}
"""

########## RunnableLambda ###########
# # chainì˜ ê²°ê³¼ê°’ì„ ë‚´ê°€ ë§Œë“  í•¨ìˆ˜ì— ì „ë‹¬í•¨

def combine_text(text):
    return text['a'].content + ' ' + text['b'].content


combine_prompt = PromptTemplate.from_template("ë‹¤ìŒì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ êµì •í•˜ê³ , ë‚´ìš©ì´ ì§€ë£¨í•˜ì§€ ì•Šë„ë¡ ì´ëª¨í‹°ì½˜ì„ ì ì ˆí•˜ê²Œ ì¶”ê°€í•´ì¤˜:\n{info} ")
print(combine_prompt)
"""
input_variables=['info'] template='ë‹¤ìŒì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ êµì •í•˜ê³ , ë‚´ìš©ì´ ì§€ë£¨í•˜ì§€ ì•Šë„ë¡ ì´ëª¨í‹°ì½˜ì„ ì ì ˆí•˜ê²Œ ì¶”ê°€í•´ì¤˜:\n{info} '
"""

# map_chainì˜ ê²°ê³¼ê°’ì´  combine_textì˜ íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ê°€ê³ ,
# í•´ë‹¹ ê²°ê³¼ê°’ì´ "info"ë¼ëŠ” keyê°’ì„ ê°€ì§„ dictionaryì˜ valueë¡œ ë“¤ì–´ê°„ í›„,
# combine_promptì™€ model chainì„ íƒ€ê²Œ ëœë‹¤
final_chain = (
        map_chain
        | {"info": RunnableLambda(combine_text)}
        | combine_prompt
        | model
)

answer = final_chain.invoke("ëŒ€í•œë¯¼êµ­")
print(answer)
"""
content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë©°, 2021ë…„ 7ì›” ê¸°ì¤€ ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ëŠ” ì•½ 5ì²œ 130ë§Œ ëª…ì´ì—ìš”! ğŸ‡°ğŸ‡·ğŸ™ï¸' response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 110, 'total_tokens': 174}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ce3c55b6-dcb4-4dad-904c-4554745a54ff-0' usage_metadata={'input_tokens': 110, 'output_tokens': 64, 'total_tokens': 174}
"""
